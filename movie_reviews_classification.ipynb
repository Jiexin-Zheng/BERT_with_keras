{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting Movie Review Sentriment with BERT\n",
    "This example is slight different with the tutorial in [**google's BERT**](https://github.com/google-research/bert#pre-trained-models) which uses pre-trained bert model. Some codes are borrowed from [this tutorial](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).\n",
    "\n",
    "The purpose of this tutorial is telling you how to use **BERT_with_keras** to pretrain an encoding model and train a classification model. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's use Standord's Large Movie Review Dataset for BERT pretraining and fine-tuning, the code below, which downloads,extracts and imports the dateset, is borrowed from this [tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub). The dataset consists of IMDB movie reviews labeled by positivity from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "  \n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                         \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                          \"aclImdb\", \"test\"))\n",
    "    return train_df, test_df\n",
    " \n",
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First I liked that movie. It seemed to me a ni...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...but this has to be the worst A Christmas Ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Sopranos stands out as an airtight, dynami...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoyed the cinematographic recreation of Ch...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'The Student of Prague' is an early feature-le...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  First I liked that movie. It seemed to me a ni...         4         0\n",
       "1  ...but this has to be the worst A Christmas Ca...         1         0\n",
       "2  The Sopranos stands out as an airtight, dynami...         9         1\n",
       "3  I enjoyed the cinematographic recreation of Ch...         8         1\n",
       "4  'The Student of Prague' is an early feature-le...         7         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "Let's use the IMDB movie reviews data for bert's pretraining. When you want to get a better **bert pre-training model**, you need to prepare data by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import random\n",
    "from const import bert_data_path,bert_model_path\n",
    "from preprocess import create_pretraining_data_from_docs\n",
    "from pretraining import bert_pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before pre-training bert model, we need to transform the movie revies data to the format we use in **Bert_with_keras model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num-0: tokens: [CLS] we also have lots of scenes with the hero fl ##au ##nting all the [MASK] of respects and protocol which the rest of the tibetan [MASK] accord ##s the dalai lama , [MASK] as we [MASK] that the hero has deep and profound [MASK] ##erence [MASK] these people and their spiritual [MASK] . [SEP] that this guy is now [MASK] buddhist , sort of , in his own way , even though we ourselves don ##Â´ ##t seem to know what his transformation en ##tails or how [MASK] [MASK] [MASK] it to go . and last but not least , we hang a stat ##istic onto the [MASK] [MASK] the film about [MASK] app ##all ##ingly the chinese have treated the tibetan ##s ( which is [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 9 15 26 33 36 44 45 46 52 60 70 88 89 90 97 107 108 109 113\n",
      "masked_lm_labels: hero marks society even pretend rev ##erence for leader a , far we want but the end of how\n",
      "\n",
      "\n",
      "num-1: tokens: [CLS] i [MASK] people writing about how great this movie was . it was horrible ! the acting [MASK] [MASK] - [MASK] at best [MASK] it made a lot of money because teenage girls went to see the movie 7 times in the theaters because of leonardo . where [MASK] hell did they get the money ? [SEP] it sank , [MASK] was running through a lot of people ' s minds effective maybe even a little [MASK] [MASK] . does anyone realize that certain restored didn ' t [MASK] [MASK] the ship because there was a fire on [MASK] before it even took off ? no , you don ' [MASK] because all you [MASK] [MASK] a rich girl falling for a poor boy and he [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 2 18 19 21 24 25 49 61 72 77 78 85 89 90 92 99 111 115 116\n",
      "masked_lm_labels: see was sub par . it the what ; conspiracy stuff people even board ship board t see is\n",
      "\n",
      "\n",
      "num-2: tokens: [CLS] any ##how , [MASK] ' s an entertaining film [MASK] if you ' ve [MASK] nothing to do on a weekday evening . [SEP] york segment of ' the day after tomorrow ' , but that stalk ' t [MASK] it any less of a film . [MASK] , the script [MASK] [MASK] ' t there . it ' ##ang merely functional , flat , and lacking [MASK] [MASK] . great british talents like [MASK] carly ##sle and david such ##et to name but two do their level best with [MASK] they ' ve got [MASK] but [MASK] characters are two - dimensional cy ##pher ##s , ##skin something out of [MASK] old marvel comic . [MASK] it ' d be frankly easier to turn back [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 4 10 15 37 40 48 52 53 60 68 69 75 84 91 96 98 108 112 117\n",
      "masked_lm_labels: it , got shouldn make however just isn s in depth robert but what , their like an and\n",
      "\n",
      "\n",
      "num-3: tokens: [CLS] directed by jacques tour ##ne ##ur ( cat people , out of the past , night of the gemini ) and [MASK] by phillip dunne ( how green was my valley ) [MASK] of the indies is a quite interesting adventure pirate movie . [SEP] pirate anne [MASK] [MASK] who actually lived and sailed through 18th century ' s atlantic . < br [MASK] > < br / > the film begins with the sea battle where anne ' s ( jean peters og [MASK] ship attacks [MASK] trade partisan that was [MASK] its way to europe from the south [MASK] . as a [MASK] a treasure [MASK] great value is captured along with a handsome french officer pierre la [MASK] ( louis jo [MASK] ##dan [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 19 22 30 31 33 48 49 64 84 85 88 90 93 101 105 108 118 121 125\n",
      "masked_lm_labels: demon written my valley anne bone ##y / ) pirate a ship on america result of officer rochelle ##ur\n",
      "\n",
      "\n",
      "num-4: tokens: [CLS] / > obviously it ' 1745 not an [MASK] contender , [MASK] it is entertaining and serves [MASK] purpose . < [MASK] / > < br [SEP] [MASK] > and for the men out there [MASK] there were an equal share [MASK] [MASK] and women in the movie theater ( most were with their wives / girlfriends ) , and the men were [MASK] fact laughing . : ) [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 6 9 12 18 22 28 36 42 43 56 64\n",
      "masked_lm_labels: s oscar but its br / , of men / in\n",
      "\n",
      "\n",
      "num-5: tokens: [CLS] bbc ' s [MASK] hour adaptation of the novel by [MASK] [MASK] . . . \" fingers ##mith \" ##anies life is tough without money , especially in dickens ##ian london . dark [MASK] lead to [MASK] ##pic ##able [MASK] ##s . is love really [MASK] [MASK] luxury for [MASK] [MASK] and free ? ? [SEP] as \" sue tri ##nder [MASK] radius give [MASK] performances as the leading ladies asking this question . . . of each other . . . whilst [MASK] evans shine ##s as the delightful ##ly [MASK] [MASK] gentleman \" . . with great support from im ##eld ##a st ##au ##nton ' s \" mrs sucks ##by \" , david trough ##ton ' s \" mr ib ##bs \" and [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 4 11 12 20 26 34 37 40 46 47 50 51 62 63 65 84 92 93 106\n",
      "masked_lm_labels: 3 sarah waters . , deeds des dilemma just a the rich \" both fantastic rupert bad \" ##au\n",
      "\n",
      "\n",
      "num-6: tokens: [CLS] the women is a cute movie about women at [MASK] ages ( but mostly 30 + ) and their issues in life . [SEP] in [MASK] ##lity bernstein but also about relationships with friends [MASK] family [MASK] making time to connect with others , [MASK] [MASK] image [MASK] com ##promising your values , [MASK] in life , and finding what will really make you happy . < br / > < br / > it ' s also about being [MASK] to yourself . a lot of [MASK] , people will give you advice but not really [MASK] it [MASK] . sometimes they have created a [MASK] ##usion for themselves , and should succeeding [MASK] follow in that same path or react [MASK] on your feelings [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 10 26 28 35 37 45 46 48 54 61 71 81 88 98 100 107 114 115 123\n",
      "masked_lm_labels: all ##fide , & , problems with , accomplishments will < true times follow themselves del you really based\n",
      "\n",
      "\n",
      "num-7: tokens: [CLS] this movie works because it feels so genuine . [MASK] story is simple and realistic , perfectly capturing [MASK] joy ##s and an ##xie ##ties of adolescent [MASK] [MASK] [MASK] that most / all of [MASK] [MASK] during our teen years . < br / > < [MASK] [SEP] and stefan the heroic vampire is about as charming as a ref [MASK] [MASK] ##ated fireplace poker , but who cares ? [MASK] ' s [MASK] one reason to watch this movie , and his name levy [MASK] ##du ! he ' [MASK] a physical homage [MASK] nos ##fera ##tu and he has the best lines in the movie [MASK] all spoken in the ras ##py voice of a man who smoke ##s ten packs of cigarettes [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 3 10 19 28 29 30 36 37 48 53 62 63 72 75 86 87 92 96 109\n",
      "masked_lm_labels: works the the love and sexuality us experienced br heroic ##ri ##ger there only is ra s to ,\n",
      "\n",
      "\n",
      "num-8: tokens: [CLS] they repeat pointless \" special effects \" so many times that it ' s obvious [MASK] were just trying to cover up the fact that they only shot 30 minutes of footage . if i were forced to [MASK] [MASK] movie on repeat [MASK] would [MASK] [MASK] ##on [MASK] unconscious with my [MASK] hands after [MASK] one and a half times through . [SEP] abu as his dog . the prince has fallen into a sleep and nothing unexpected wake her [MASK] so ja ##ffa ##r sends his servant hal ##ima for ahmad and the dog , in hopes the prince can rouse [MASK] [MASK] he does awake ##n [MASK] . she boards [MASK] ship to find [MASK] doctor to [MASK] ahmad , but [MASK] is [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 16 39 40 44 46 47 49 53 56 66 79 82 104 105 110 114 118 121 125\n",
      "masked_lm_labels: they watch this i blu ##dge myself own about as can . her . her a a cure she\n",
      "\n",
      "\n",
      "num-9: tokens: [CLS] spy when men clear the way so none will [MASK] the princess of the city passing by . ahmad falls in love with [MASK] and visits her in her garden . he tells leon he has come to her from beyond [MASK] [MASK] wins a [MASK] [MASK] then [MASK] is captured . when ja ##ffa ##r comes to win the princess of bas ##ra for himself , ahmad attacks the evil viz ##ier who blinds him and turns [SEP] ja ##ffa ##r [MASK] asks for the ##ap [MASK] s hand , and he gives the gift of a mechanical [MASK] horse [MASK] the sultan of bas ##ra . ##film blind [MASK] then tells his canonical in the [MASK] , accompanied by abu as his dog . [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 10 24 34 42 43 46 47 49 54 83 87 88 100 102 104 109 111 115 118\n",
      "masked_lm_labels: see her her time and kiss . he ja then princess ' flying to sultan the ahmad tale marketplace\n",
      "\n",
      "\n",
      "[INFO] number of train data: 1059\n",
      "[INFO] is_random_next ratio: 0.5684608120868744\n"
     ]
    }
   ],
   "source": [
    "# use spacy for sentence split\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# use IMDB movie review as pretraining data\n",
    "texts = train['sentence'].tolist() + test['sentence'].tolist()\n",
    "\n",
    "# To keep training fast on my macbook, I take a sample of 50 movie reviews. \n",
    "# Due to this operation, pretraining model may be very bad. \n",
    "# So you can try with all movie reviews to get better pre-training model.\n",
    "random.shuffle(texts)\n",
    "texts = texts[0:50]\n",
    "\n",
    "# sentence split\n",
    "sentences_texts=[]\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    sentences_texts.append([s.text for s in doc.sents])\n",
    "\n",
    "vocab_path = os.path.join(bert_data_path, 'vocab.txt')\n",
    "\n",
    "# set dupe_factor=5 to reduce the samples of pre-training data. defaut:10\n",
    "create_pretraining_data_from_docs(sentences_texts,\n",
    "                                  vocab_path=vocab_path,\n",
    "                                  save_path=os.path.join(bert_data_path,'pretraining_data.npz'),\n",
    "                                  token_method='wordpiece',\n",
    "                                  language='en',\n",
    "                                  dupe_factor=5\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared the pre-training data, let's focus on training a pre-training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhongan/Documents/competition/BERT_with_keras/pretraining.py:107: UserWarning: model performance may be suitable when warmup steps is 0.01~0.02 of train steps.\n",
      "  warnings.warn(\"model performance may be suitable when warmup steps is 0.01~0.02 of train steps.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "119/120 [============================>.] - ETA: 14s - loss: 3.3843 - lm_model_loss: 6.0340 - next_sentence_model_loss: 0.7346 - lm_model_acc: 0.2309 - next_sentence_model_acc: 0.5158\n",
      "step 0000120: cur_lm_acc is 0.25802, cur_is_random_nex_acc is 0.56604\n",
      "\n",
      "Step 0000120: val_acc improved from -inf to 0.41203, saving model to /Users/zhongan/Documents/competition/BERT_with_keras/models/bert_pretraining_movie_reviews.h5\n",
      "120/120 [==============================] - 1927s 16s/step - loss: 3.3834 - lm_model_loss: 6.0344 - next_sentence_model_loss: 0.7324 - lm_model_acc: 0.2299 - next_sentence_model_acc: 0.5198 - val_loss: 2.9744 - val_lm_model_loss: 5.2540 - val_next_sentence_model_loss: 0.6949 - val_lm_model_acc: 0.2580 - val_next_sentence_model_acc: 0.5660\n",
      "Epoch 2/2\n",
      "110/120 [==========================>...] - ETA: 2:19 - loss: 2.9011 - lm_model_loss: 5.1127 - next_sentence_model_loss: 0.6895 - lm_model_acc: 0.2390 - next_sentence_model_acc: 0.5659\n",
      "step 0000230: cur_lm_acc is 0.25142, cur_is_random_nex_acc is 0.56604\n",
      "\n",
      "Step 0000230: val_acc did not improve from 0.41203, current is 0.40873\n",
      "120/120 [==============================] - 1779s 15s/step - loss: 2.8996 - lm_model_loss: 5.1107 - next_sentence_model_loss: 0.6886 - lm_model_acc: 0.2391 - next_sentence_model_acc: 0.5698 - val_loss: 2.9544 - val_lm_model_loss: 5.2242 - val_next_sentence_model_loss: 0.6846 - val_lm_model_acc: 0.2514 - val_next_sentence_model_acc: 0.5660\n"
     ]
    }
   ],
   "source": [
    "# pretraining a bert encoder model\n",
    "bert_pretraining(train_data_path=os.path.join(bert_data_path,'pretraining_data.npz'),\n",
    "                 bert_config_file=os.path.join(bert_data_path, 'bert_config.json'),\n",
    "                 save_path=bert_model_path,\n",
    "                 batch_size=8,\n",
    "                 seq_length=128,\n",
    "                 max_predictions_per_seq=20,\n",
    "                 val_batch_size=128,\n",
    "                 multi_gpu=0,\n",
    "                 num_warmup_steps=10,\n",
    "                 checkpoints_interval_steps=110,\n",
    "                 max_num_val=1000,\n",
    "                 pretraining_model_name='bert_pretraining_movie_reviews.h5',\n",
    "                 encoder_model_name='bert_movie_reviews_encoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fune-tuning\n",
    "Use the above pre-training model as the initial point for your NLP model.Here the pre-training model is used to classify the movie reviews(i.e. classifying whether a movie review is positive or negtive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from const import bert_data_path, bert_model_path\n",
    "from modeling import BertConfig\n",
    "from classifier import SingleSeqDataProcessor, convert_examples_to_features, text_classifier, save_features\n",
    "from tokenization import FullTokenizer\n",
    "from optimization import AdamWeightDecayOpt\n",
    "from checkpoint import StepModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert movie reviews data to what we need in bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_examples = SingleSeqDataProcessor.get_train_examples(train_data=train['sentence'].tolist(),labels=train['polarity'].tolist())\n",
    "dev_exmaples = SingleSeqDataProcessor.get_dev_examples(dev_data=test['sentence'].tolist(), labels=test['polarity'].tolist())\n",
    "\n",
    "# `word piece tokenizer` need to a prepared vocabulary.\n",
    "vocab_path = os.path.join(bert_data_path, 'vocab.txt')\n",
    "\n",
    "# load vocab to tokenizer\n",
    "tokenizer = FullTokenizer(vocab_path, do_lower_case=True)\n",
    "\n",
    "# convert the train and dev examples to features\n",
    "train_features = convert_examples_to_features(train_examples, \n",
    "                                              label_list=[0,1], \n",
    "                                              max_seq_length=128, \n",
    "                                              tokenizer= tokenizer)\n",
    "dev_features = convert_examples_to_features(dev_exmaples, label_list=[0,1], max_seq_length=128, tokenizer=tokenizer)\n",
    "\n",
    "# convert features to a dictionary of numpy arrays.\n",
    "train_features_array_dict = save_features(features=train_features)\n",
    "dev_features_array_dict = save_features(features=dev_features)\n",
    "\n",
    "# get train and validation data\n",
    "train_x = [train_features_array_dict['input_ids'], train_features_array_dict['input_mask'], train_features_array_dict['segment_ids']]\n",
    "train_y = keras.utils.to_categorical(train_features_array_dict['label_ids'], 2)\n",
    "val_x = [dev_features_array_dict['input_ids'], dev_features_array_dict['input_mask'], dev_features_array_dict['segment_ids']]\n",
    "val_y = keras.utils.to_categorical(dev_features_array_dict['label_ids'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secend, we need to set the args of bert model. we also need to define a opimizer and a checkponter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train steps: 4689\n"
     ]
    }
   ],
   "source": [
    "# load bert configuration file\n",
    "config = BertConfig.from_json_file(os.path.join(bert_data_path, 'bert_config.json'))\n",
    "epochs = 3\n",
    "num_gpus = None\n",
    "# if you come across a OOM problem, reduce the batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# calculation the number of training steps by epoch size.\n",
    "num_train_samples = len(train_features_array_dict['input_ids'])\n",
    "num_train_steps = int(np.ceil(num_train_samples / batch_size)) * epochs\n",
    "print(\"number of train steps: {}\".format(num_train_steps))\n",
    "\n",
    "# Use weight decay adam optimizer. this optimizer is sightly different with Keras's Standard Adam optimizer. \n",
    "# For more details, view source code of AdamWeightDecayOpt.\n",
    "adam = AdamWeightDecayOpt(\n",
    "        lr=5e-5,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=100,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-6,\n",
    "        weight_decay_rate=0.01,\n",
    "        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"]\n",
    "    )\n",
    "\n",
    "# This checkpoint evaluate the bert model performance on batch end.\n",
    "checkpoint = StepModelCheckpoint(filepath=\"%s/%s\" % (bert_model_path, 'imdb_classifer_model.h5'),\n",
    "                                 verbose=1, monitor='val_acc',\n",
    "                                 save_best_only=True,\n",
    "                                 xlen=3,\n",
    "                                 period=100,\n",
    "                                 start_step=100,\n",
    "                                 val_batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, create a bert classification model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " 2432/25000 [=>............................] - ETA: 4:42:21 - loss: 0.7182"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "classifier = text_classifier(bert_config=config,\n",
    "                             pretrain_model_path=os.path.join(bert_model_path, 'bert_movie_reviews_encoder.h5'),\n",
    "                             batch_size=batch_size,\n",
    "                             seq_length=128,\n",
    "                             optimizer=adam,\n",
    "                             num_classes=2,\n",
    "                             multi_gpu= num_gpus\n",
    "                             )\n",
    "\n",
    "# when using multi-gpus, the parallel model of bert cann't be used to evaluate/predict.\n",
    "# You can only use the cpu_build model to evalate and predict.\n",
    "if num_gpus is not None:\n",
    "    checkpoint.single_gpu_model = classifier.model\n",
    "\n",
    "# train model\n",
    "history = classifier.fit(x=train_x,\n",
    "                         y=train_y,\n",
    "                         epochs=epochs,\n",
    "                         shuffle=True,\n",
    "                         callbacks=[checkpoint],\n",
    "                         validation_data=(val_x,val_y)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
